"""
TokenRecæµ‹è¯•è„šæœ¬ - é€æ­¥éªŒè¯æ¯ä¸ªæ¨¡å—
è¿è¡Œæ­¤è„šæœ¬æ¥æ£€æŸ¥ä»£ç æ˜¯å¦å¯ä»¥æ­£å¸¸å·¥ä½œ
"""

import torch
import numpy as np
import sys
import os
from datetime import datetime

def print_section(title):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–åŒ…"""
    print_section("Step 0: æ£€æŸ¥ä¾èµ–åŒ…")
    
    required_packages = {
        'torch': 'PyTorch',
        'numpy': 'NumPy',
        'pandas': 'Pandas',
        'transformers': 'Hugging Face Transformers',
        'tqdm': 'tqdm'
    }
    
    missing = []
    for package, name in required_packages.items():
        try:
            __import__(package)
            print(f"âœ“ {name:30s} - å·²å®‰è£…")
        except ImportError:
            print(f"âœ— {name:30s} - æœªå®‰è£…")
            missing.append(package)
    
    if missing:
        print(f"\nâš ï¸  ç¼ºå°‘ä¾èµ–åŒ…: {', '.join(missing)}")
        print("è¯·è¿è¡Œ: pip install " + " ".join(missing))
        return False
    
    # æ£€æŸ¥CUDA
    if torch.cuda.is_available():
        print(f"\nâœ“ CUDA å¯ç”¨")
        print(f"  GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"  GPUåç§°: {torch.cuda.get_device_name(0)}")
    else:
        print(f"\nâš ï¸  CUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU (è®­ç»ƒä¼šå¾ˆæ…¢)")
    
    return True

def test_mq_tokenizer():
    """æµ‹è¯•MQ-Tokenizer"""
    print_section("Step 1: æµ‹è¯•MQ-Tokenizer")
    
    try:
        from tokenrec_core import MQTokenizer
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 16
        emb_dim = 64
        test_embeddings = torch.randn(batch_size, emb_dim)
        
        print(f"è¾“å…¥å½¢çŠ¶: {test_embeddings.shape}")
        
        # åˆå§‹åŒ–tokenizer
        tokenizer = MQTokenizer(
            input_dim=emb_dim,
            K=3,
            L=128,  # å‡å°ä»¥åŠ å¿«æµ‹è¯•
            d_c=32,
            mask_ratio=0.2
        )
        
        print(f"âœ“ MQTokenizeråˆå§‹åŒ–æˆåŠŸ")
        print(f"  å‚æ•°é‡: {sum(p.numel() for p in tokenizer.parameters()):,}")
        
        # å‰å‘ä¼ æ’­
        tokens, reconstructed, loss_dict = tokenizer(test_embeddings)
        
        print(f"âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"  Tokenså½¢çŠ¶: {tokens.shape}")
        print(f"  é‡å»ºå½¢çŠ¶: {reconstructed.shape}")
        print(f"  é‡å»ºæŸå¤±: {loss_dict['recon_loss'].item():.4f}")
        print(f"  ç æœ¬æŸå¤±: {loss_dict['codebook_loss'].item():.4f}")
        print(f"  æ‰¿è¯ºæŸå¤±: {loss_dict['commitment_loss'].item():.4f}")
        
        # æµ‹è¯•è®­ç»ƒæ­¥éª¤
        optimizer = torch.optim.Adam(tokenizer.parameters(), lr=1e-3)
        loss = loss_dict['recon_loss'] + loss_dict['codebook_loss'] + 0.25 * loss_dict['commitment_loss']
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        print(f"âœ“ åå‘ä¼ æ’­æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âœ— MQTokenizeræµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_tokenrec_model():
    """æµ‹è¯•TokenRecæ¨¡å‹"""
    print_section("Step 2: æµ‹è¯•TokenRecæ¨¡å‹")
    
    try:
        from tokenrec_core import MQTokenizer, TokenRec
        
        # æ¨¡æ‹Ÿå‚æ•°
        num_users = 100
        num_items = 50
        emb_dim = 64
        
        # åˆ›å»ºtokenizers
        user_tokenizer = MQTokenizer(emb_dim, K=2, L=64, d_c=32)
        item_tokenizer = MQTokenizer(emb_dim, K=2, L=64, d_c=32)
        
        print(f"âœ“ Tokenizersåˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºTokenRec (ä½¿ç”¨å°æ¨¡å‹æµ‹è¯•)
        print("æ­£åœ¨åŠ è½½T5æ¨¡å‹ (å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
        model = TokenRec(
            user_tokenizer,
            item_tokenizer,
            llm_model_name='t5-small',
            item_emb_dim=emb_dim
        )
        
        print(f"âœ“ TokenRecåˆå§‹åŒ–æˆåŠŸ")
        print(f"  æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        batch_size = 4
        user_emb = torch.randn(batch_size, emb_dim)
        
        # ä¸ä½¿ç”¨å†å²
        print("\næµ‹è¯•1: ä¸ä½¿ç”¨äº¤äº’å†å²")
        z = model(user_emb, None)
        print(f"âœ“ è¾“å‡ºå½¢çŠ¶: {z.shape}")
        
        # ä½¿ç”¨å†å²
        print("\næµ‹è¯•2: ä½¿ç”¨äº¤äº’å†å²")
        seq_len = 5
        item_history = torch.randn(batch_size, seq_len, emb_dim)
        z = model(user_emb, item_history)
        print(f"âœ“ è¾“å‡ºå½¢çŠ¶: {z.shape}")
        
        # æµ‹è¯•æ£€ç´¢
        print("\næµ‹è¯•3: Top-Kæ£€ç´¢")
        item_database = torch.randn(num_items, emb_dim)
        top_k_indices, top_k_scores = model.retrieve_top_k(z, item_database, k=10)
        print(f"âœ“ Top-Kå½¢çŠ¶: {top_k_indices.shape}")
        print(f"  æ£€ç´¢çš„ç‰©å“ID: {top_k_indices[0].tolist()}")
        
        return True
        
    except Exception as e:
        print(f"âœ— TokenRecæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_lightgcn():
    """æµ‹è¯•LightGCN"""
    print_section("Step 3: æµ‹è¯•LightGCN")
    
    try:
        from lightgcn import LightGCN
        
        num_users = 100
        num_items = 50
        emb_dim = 32
        
        # åˆ›å»ºæ¨¡å‹
        model = LightGCN(
            num_users=num_users,
            num_items=num_items,
            embedding_dim=emb_dim,
            num_layers=2
        )
        
        print(f"âœ“ LightGCNåˆå§‹åŒ–æˆåŠŸ")
        print(f"  å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # åˆ›å»ºæµ‹è¯•é‚»æ¥çŸ©é˜µ
        total_nodes = num_users + num_items
        num_edges = 200
        
        # éšæœºç”Ÿæˆè¾¹
        row = torch.randint(0, total_nodes, (num_edges,))
        col = torch.randint(0, total_nodes, (num_edges,))
        values = torch.ones(num_edges)
        
        adj_matrix = torch.sparse_coo_tensor(
            torch.stack([row, col]),
            values,
            (total_nodes, total_nodes)
        )
        
        print(f"âœ“ é‚»æ¥çŸ©é˜µåˆ›å»ºæˆåŠŸ: {adj_matrix.shape}")
        
        # å‰å‘ä¼ æ’­
        user_emb, item_emb = model(adj_matrix)
        
        print(f"âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"  ç”¨æˆ·embeddings: {user_emb.shape}")
        print(f"  ç‰©å“embeddings: {item_emb.shape}")
        
        # æµ‹è¯•BPRæŸå¤±
        users = torch.randint(0, num_users, (16,))
        pos_items = torch.randint(0, num_items, (16,))
        neg_items = torch.randint(0, num_items, (16,))
        
        bpr_loss = model.bpr_loss(users, pos_items, neg_items, user_emb, item_emb)
        reg_loss = model.reg_loss(users, pos_items, neg_items)
        
        print(f"âœ“ æŸå¤±è®¡ç®—æˆåŠŸ")
        print(f"  BPR Loss: {bpr_loss.item():.4f}")
        print(f"  Reg Loss: {reg_loss.item():.4f}")
        
        return True
        
    except Exception as e:
        print(f"âœ— LightGCNæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_data_loader():
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    print_section("Step 4: æµ‹è¯•æ•°æ®åŠ è½½")
    
    try:
        from amazon_loader import AmazonDataLoader, BipartiteGraph
        
        print("æ³¨æ„: æ­¤æ­¥éª¤éœ€è¦ä¸‹è½½Amazonæ•°æ®é›†")
        print("å¦‚æœæ•°æ®é›†ä¸å­˜åœ¨ï¼Œå°†è·³è¿‡æ­¤æµ‹è¯•\n")
        
        loader = AmazonDataLoader(category='Beauty', data_dir='./tokenrec_project/data')
        
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(loader.raw_data_path):
            print(f"âš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {loader.raw_data_path}")
            print("è¯·æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†æˆ–è¿è¡Œä¸‹è½½å‡½æ•°")
            return None
        
        # åŠ è½½æ•°æ®
        data = loader.load_and_preprocess()
        
        print(f"âœ“ æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"  ç”¨æˆ·æ•°: {data['num_users']}")
        print(f"  ç‰©å“æ•°: {data['num_items']}")
        print(f"  è®­ç»ƒäº¤äº’: {len(data['train_df'])}")
        
        # æ„å»ºå›¾
        graph = BipartiteGraph(
            num_users=data['num_users'],
            num_items=data['num_items'],
            train_interactions=data['train_interactions']
        )
        
        adj_matrix, edge_index = graph.to_torch_sparse_coo()
        
        print(f"âœ“ å›¾æ„å»ºæˆåŠŸ")
        print(f"  é‚»æ¥çŸ©é˜µ: {adj_matrix.shape}")
        print(f"  è¾¹æ•°é‡: {edge_index.shape[1]}")
        
        return True
        
    except Exception as e:
        print(f"âš ï¸  æ•°æ®åŠ è½½æµ‹è¯•è·³è¿‡: {str(e)}")
        return None

def test_mini_pipeline():
    """æµ‹è¯•å°è§„æ¨¡ç«¯åˆ°ç«¯æµç¨‹"""
    print_section("Step 5: ç«¯åˆ°ç«¯å°è§„æ¨¡æµ‹è¯•")
    
    try:
        from tokenrec_core import MQTokenizer, TokenRec
        
        print("åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®...")
        
        # å°è§„æ¨¡æ•°æ®
        num_users = 50
        num_items = 30
        emb_dim = 32
        
        # æ¨¡æ‹ŸGNN embeddings
        user_embeddings = torch.randn(num_users, emb_dim)
        item_embeddings = torch.randn(num_items, emb_dim)
        
        print(f"âœ“ æ•°æ®åˆ›å»ºæˆåŠŸ")
        
        # æ­¥éª¤1: è®­ç»ƒUser Tokenizer
        print("\n1. è®­ç»ƒUser Tokenizer (10 epochs)...")
        user_tokenizer = MQTokenizer(emb_dim, K=2, L=32, d_c=16, mask_ratio=0.2)
        optimizer = torch.optim.Adam(user_tokenizer.parameters(), lr=1e-3)
        
        for epoch in range(10):
            tokens, recon, losses = user_tokenizer(user_embeddings)
            loss = losses['recon_loss'] + losses['codebook_loss'] + 0.25 * losses['commitment_loss']
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            if (epoch + 1) % 5 == 0:
                print(f"  Epoch {epoch+1}: Loss = {loss.item():.4f}")
        
        print("âœ“ User Tokenizerè®­ç»ƒå®Œæˆ")
        
        # æ­¥éª¤2: è®­ç»ƒItem Tokenizer
        print("\n2. è®­ç»ƒItem Tokenizer (10 epochs)...")
        item_tokenizer = MQTokenizer(emb_dim, K=2, L=32, d_c=16, mask_ratio=0.2)
        optimizer = torch.optim.Adam(item_tokenizer.parameters(), lr=1e-3)
        
        for epoch in range(10):
            tokens, recon, losses = item_tokenizer(item_embeddings)
            loss = losses['recon_loss'] + losses['codebook_loss'] + 0.25 * losses['commitment_loss']
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            if (epoch + 1) % 5 == 0:
                print(f"  Epoch {epoch+1}: Loss = {loss.item():.4f}")
        
        print("âœ“ Item Tokenizerè®­ç»ƒå®Œæˆ")
        
        # æ­¥éª¤3: åˆ›å»ºTokenRec
        print("\n3. åˆ›å»ºTokenRecæ¨¡å‹...")
        model = TokenRec(user_tokenizer, item_tokenizer, 
                        llm_model_name='t5-small', item_emb_dim=emb_dim)
        
        # å†»ç»“tokenizers
        for param in model.user_tokenizer.parameters():
            param.requires_grad = False
        for param in model.item_tokenizer.parameters():
            param.requires_grad = False
        
        print("âœ“ TokenRecåˆ›å»ºæˆåŠŸ")
        
        # æ­¥éª¤4: æµ‹è¯•è®­ç»ƒæ­¥éª¤
        print("\n4. æµ‹è¯•è®­ç»ƒæ­¥éª¤ (5 steps)...")
        optimizer = torch.optim.Adam([
            {'params': model.llm.parameters(), 'lr': 1e-4},
            {'params': model.projection.parameters(), 'lr': 1e-3}
        ])
        
        batch_size = 8
        for step in range(5):
            # éšæœºé‡‡æ ·
            user_idx = torch.randint(0, num_users, (batch_size,))
            pos_idx = torch.randint(0, num_items, (batch_size,))
            neg_idx = torch.randint(0, num_items, (batch_size,))
            
            user_emb = user_embeddings[user_idx]
            pos_emb = item_embeddings[pos_idx]
            neg_emb = item_embeddings[neg_idx]
            
            # å‰å‘ä¼ æ’­
            z = model(user_emb, None)
            
            # è®¡ç®—æŸå¤±
            loss = model.compute_ranking_loss(z, pos_emb, neg_emb, margin=0.1)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            print(f"  Step {step+1}: Loss = {loss.item():.4f}")
        
        print("âœ“ è®­ç»ƒæ­¥éª¤æµ‹è¯•å®Œæˆ")
        
        # æ­¥éª¤5: æµ‹è¯•æ¨ç†
        print("\n5. æµ‹è¯•æ¨ç†...")
        model.eval()
        with torch.no_grad():
            test_user_emb = user_embeddings[:5]
            z = model(test_user_emb, None)
            top_k_indices, top_k_scores = model.retrieve_top_k(z, item_embeddings, k=10)
        
        print(f"âœ“ æ¨ç†æˆåŠŸ")
        print(f"  ä¸º{len(test_user_emb)}ä¸ªç”¨æˆ·æ¨èäº†top-10ç‰©å“")
        print(f"  ç¤ºä¾‹æ¨è: {top_k_indices[0].tolist()}")
        
        return True
        
    except Exception as e:
        print(f"âœ— ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "=" * 70)
    print("  TokenRec ä»£ç æµ‹è¯•è„šæœ¬")
    print("  å¼€å§‹æ—¶é—´:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("=" * 70)
    
    results = {}
    
    # 0. æ£€æŸ¥ä¾èµ–
    results['dependencies'] = check_dependencies()
    if not results['dependencies']:
        print("\nâŒ ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œè¯·å…ˆå®‰è£…ç¼ºå¤±çš„åŒ…")
        return
    
    # 1. æµ‹è¯•MQ-Tokenizer
    results['mq_tokenizer'] = test_mq_tokenizer()
    
    # 2. æµ‹è¯•TokenRec
    results['tokenrec'] = test_tokenrec_model()
    
    # 3. æµ‹è¯•LightGCN
    results['lightgcn'] = test_lightgcn()
    
    # 4. æµ‹è¯•æ•°æ®åŠ è½½ (å¯é€‰)
    results['data_loader'] = test_data_loader()
    
    # 5. ç«¯åˆ°ç«¯æµ‹è¯•
    results['end_to_end'] = test_mini_pipeline()
    
    # æ€»ç»“
    print_section("æµ‹è¯•æ€»ç»“")
    
    for test_name, result in results.items():
        if result is True:
            status = "âœ“ é€šè¿‡"
        elif result is False:
            status = "âœ— å¤±è´¥"
        else:
            status = "âŠ˜ è·³è¿‡"
        
        print(f"{test_name:20s}: {status}")
    
    passed = sum(1 for r in results.values() if r is True)
    total = len([r for r in results.values() if r is not None])
    
    print(f"\né€šè¿‡ç‡: {passed}/{total}")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä»£ç å¯ä»¥æ­£å¸¸è¿è¡Œã€‚")
        print("   ç°åœ¨å¯ä»¥è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹ã€‚")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤ã€‚")
    
    print("\nç»“æŸæ—¶é—´:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))

if __name__ == "__main__":
    main()